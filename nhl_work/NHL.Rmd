---
title: "Data Science semester project"
subtitle: "Predicting goals scored in a season by NHL teams"
author: "Barát Dóra and Preisinger Péter"
date: '2019.12.19. - 2020.01.08.'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(ggplot2)
library(dplyr)
library(magrittr)
library(caret)
library(glmnet)
set.seed(123456789)
```
## The big idea

- We had data from NHL players
  - NHL is the north american first league for hockey; NHL stands for National Hockey League
- We wanted to predict the goals scored by a given team based on the player statistics
- Yes, we are aware that using team data would have been easier, but our goal is to get macro results from micro data

## Data cleaning

- During this step we converted data types to numeric where neccessary
- We also replaced *NA*-s with *0*-s where it made sense
- During this step we also decided that face-off statistics would not be used so we didn't bother to convert those
- And finally we removed lines where there was data missing *(such as a person with age = 0)*
- We also deleted lines referring to the team *TOT* as that is just aggregated data for players
that changed teams during season
- After all of this we grouped the data according to teams
- And made column names easier to understand
- We also dropped some variables that seemed meaningless in an agragated sense

```{r data cleaning and organizing, warning = FALSE}
# reading in the data
skater_stats <- fread("../nhl_data/skater_stats.csv")

# cleaning the data
skater_stats$V1 <- NULL
setnames(skater_stats, c("+/-", "S%", "FO%"),
         c("PlusMinus", "S_perc", "FO_perc"))

skater_stats <- skater_stats[Season >= 2008]

cols_to_numeric <- c("G", "PTS", "PlusMinus", "A",
                     "PIM", "EVG", "PPG", "SHG", "GWG",
                     "EVA", "PPA", "SHA", "S", "S_perc")

cols_na2zero <- c("G", "PTS", "PlusMinus", "A", "PIM",
                  "EVG", "PPG", "SHG", "GWG", "EVA",
                  "PPA", "SHA", "S", "BLK", "HIT")

# as some columns are not in numeric format we changed them where sensible
skater_stats[, (cols_to_numeric) := lapply(.SD, as.numeric), .SDcols = cols_to_numeric]

# the columns working with on ice time need to be converted as well
skater_stats$TOI = gsub(",", "", skater_stats$TOI)
skater_stats[, TOI := as.numeric(TOI)]
skater_stats[, ATOI := TOI/GP]

# removing unnecesary NA
skater_stats[is.na(skater_stats)] <- 0
skater_stats <- skater_stats[!(Tm == "TOT")]
skater_stats <- skater_stats[(Age > 0)]

# selecting, grouping and reorganizing data columns that will be used
data <- skater_stats %>%
  group_by(Season, Tm) %>%
  summarise(Goals = sum(G),
            Age = mean(Age),
            GamesPlayed = mean(GP),
            Game_per_goal = mean(GPG),
            Assists = sum(A),
            Points = sum(PTS),
            PlusMinus_sum = sum(PlusMinus),
            PlusMinus_mean = mean(PlusMinus),
            Penalty_minutes = sum(PIM),
            Shots = sum(S),
            Time_on_ice = mean(TOI),
            Average_time_on_ice = mean(ATOI),
            Blocks = sum(BLK),
            Hits = sum(HIT)
            )
```

## Prediction

- We decided to use Ridge regression as there are a lot of high collinear variables in our dataset
- For this part we created the teaching and test groups (80:20)
- We tried to remove variables which were basically recording the same data *(PlusMinus_mean - PlusMinus_sum)*

## The meaning of the variables

- *Age* shows how much experience matters
- *GamesPlayed* is there to account for the difference in games for those who made playoffs or had to play extra games for other reasons
- *Assists* and *Points* are used as a measure of effectiveness, but later will be removed as the two of them perfectly define the number of goals scored with an easy substraction
- *PlusMinus_mean* is an estimation of the teams goals scored for vs goals scored against statistics. This can be used to instead of *Assists* and *Points* for effectivness without making it easy to get the number of goals scored
- *Penalty_minutes* is used to measure how fairly a team plays. It is also there as a control because a higher than average number means that the team probably played more short-handed (with one less player on ice) than other teams which would probably decrease the chances of scoring
- *Shots* is used to show how offensively a team playes
- *Average_time_on_ice* helps us to estimate how much players are rotated in a team. Smaller numbers mean that more players got a chance to be on ice per game.
- *Blocks* is used to show the defensive capabilities of a team
- *Hits* shows how much physical contact there is when the team is playing

```{r dividing the data}
# creating teaching and test data
index <- sample(1:331, 66)
test_data <- data[index,]
teach_data <- data[-index,]

ggplot(teach_data,aes(Goals)) + geom_histogram(bins = 50) + 
  geom_histogram(data = test_data, aes(alpha = 0.5, fill = "red"), bins = 50, show.legend = FALSE) +
  theme_minimal() + ggtitle("Distribution of teach and test datasets") +
  theme(plot.title = element_text(hjust=0.5))
# grey is teach_data, red is test_data
```

```{r correlation matrix, include = FALSE}
# creating correlation matrix
temp_teach <- subset(teach_data, select = -c(Season, Tm))
cor(temp_teach)
```

```{r 1st model}
# creating the data matrix for the predictor
x_var <- data.matrix(teach_data[, c("Age", "GamesPlayed", "Assists", "Points",
                                    "PlusMinus_mean", "Penalty_minutes", "Shots", 
                                    "Average_time_on_ice", "Blocks", "Hits")])
y_var <- data.matrix(teach_data[, "Goals"])

model_data <- as.data.frame(cbind(x_var, y_var))
names(model_data) <- c("Age", "GamesPlayed", "Assists", "Points", 
                       "PlusMinus_mean", "Penalty_minutes", "Shots", 
                       "Average_time_on_ice", "Blocks", "Hits", "Goals")

# searching for the best model
lambda_seq <- 10^seq(2, -2, by = -.1)
fit <- glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
ridge_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
best_fit <- ridge_cv$glmnet.fit
best_ridge <- glm(Goals ~., data = model_data)

# testing with the test data
test_data$PredictedGoals <- predict(best_ridge, newdata = test_data)
actual <- test_data$Goals
preds <- test_data$PredictedGoals
rss <- sum((preds - actual)^2)
tss <- sum((actual - mean(actual))^2)
rsq <- 1 - rss/tss
rsq

summary(best_ridge)

# showing the difference between the prediction and the real numbers
all.equal(test_data$Goals, test_data$PredictedGoals)
ggplot(test_data, aes(Goals, PredictedGoals)) + geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() + theme(plot.title = element_text(hjust=0.5)) +
  ggtitle("Difference between predicted and real data")

```

- As the r squared value was 1 we suspected there might be variables that allow for perfect prediction
- After a while we realized that indeed that was the case, *(Substracting the number of Assists from the number of Points you get the number of Goals)* so we removed one of these varibles to give the model a challange
- First we removed *Assists* as it is easier to get the number of goals from the number of assists (0, 1 or 2 assists are gained for each goal depending on the play beforehand)

```{r w/out Assists}
# modifying the dataset (removing Assists)
x_var <- data.matrix(teach_data[, c("Age", "GamesPlayed", "Points",
                                    "PlusMinus_mean", "Penalty_minutes", "Shots", 
                                    "Average_time_on_ice", "Blocks", "Hits")])
model_data$Assists <- NULL

# searching for the best model
fit <- glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
ridge_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
best_fit <- ridge_cv$glmnet.fit
best_ridge <- glm(Goals ~., data = model_data)

# testing with the test data
test_data$PredictedGoals <- predict(best_ridge, newdata = test_data)
actual <- test_data$Goals
preds <- test_data$PredictedGoals
rss <- sum((preds - actual)^2)
tss <- sum((actual - mean(actual))^2)
rsq <- 1 - rss/tss
rsq

summary(best_ridge)

# showing the difference between the prediction and the real numbers
all.equal(test_data$Goals, test_data$PredictedGoals)
ggplot(test_data, aes(Goals, PredictedGoals)) + geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() + theme(plot.title = element_text(hjust=0.5)) +
  ggtitle("Difference between predicted and real data")
```

- As the model was still heavily dependent on the *Points* stat and it is highly correlated with the number of goals scored we tried to create one without the *Points* stat

```{r w/out Assists and Points}
# modifying the dataset (removing Points)
x_var <- data.matrix(teach_data[, c("Age", "GamesPlayed","PlusMinus_mean",
                                    "Penalty_minutes", "Shots", 
                                    "Average_time_on_ice", "Blocks", "Hits")])
model_data$Points <- NULL

# searching for the best model
fit <- glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
ridge_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
best_fit <- ridge_cv$glmnet.fit
best_ridge <- glm(Goals ~., data = model_data)

# testing with the test data
test_data$PredictedGoals <- predict(best_ridge, newdata = test_data)
actual <- test_data$Goals
preds <- test_data$PredictedGoals
rss <- sum((preds - actual)^2)
tss <- sum((actual - mean(actual))^2)
rsq <- 1 - rss/tss
rsq

summary(best_ridge)

# showing the difference between the prediction and the real numbers
all.equal(test_data$Goals, test_data$PredictedGoals)
ggplot(test_data, aes(Goals, PredictedGoals)) + geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  theme_minimal() + theme(plot.title = element_text(hjust=0.5)) +
  ggtitle("Difference between predicted and real data")
```

## Final model

- We decided to use this as the final model as had the highest number of significant variables while still being accurate
- Although some variables are still not significant in our model we believe we shouldn't leave them out as they still carry important information regarding the teams
