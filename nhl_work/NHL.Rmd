---
title: "NHL"
author: "Preisinger Péter"
date: '2019 12 19 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(ggplot2)
library(dplyr)
library(magrittr)
library(caret)
library(glmnet)
set.seed(123456789)
```
## The big idea

- We had data from NHL players and the final result for the seasons
- We wanted to predict the goals scored by a given team based on the player statistics
- Yes, we are aware that using team data would have been easier, but our goal is to get macro results from micro data


## Data cleaning

- During this step we converted data types to numeric where neccessary
- We also replaced *NA*-s with *0*-s where it made sense
- And finally we removed lines where there was data missing *(such as a person with age = 0)*
- We also deleted lines referring to the team *TOT* as that is just aggregated data for players
that changed teams during season
- After all of this we grouped the data according to teams
- And made column names easier to understand
- We also dropped some variables that seemed meaningless in an agragated sense


```{r data cleaning and organizing, echo = FALSE, warning = FALSE}
skater_stats <- fread("../nhl_data/skater_stats.csv")
skater_stats$V1 <- NULL
setnames(skater_stats, c("+/-", "S%", "FO%"), c("PlusMinus", "S_perc", "FO_perc"))
skater_stats <- skater_stats[Season >= 2008]
cols_to_convert <- c("G", "PTS", "PlusMinus", "A", "PIM", "EVG", "PPG", "SHG", "GWG", "EVA", "PPA", "SHA", "S", "S_perc")
cols_na2zero <- c("G", "PTS", "PlusMinus", "A", "PIM", "EVG", "PPG", "SHG", "GWG", "EVA", "PPA", "SHA", "S", "BLK", "HIT")
skater_stats[, (cols_to_convert) := lapply(.SD, as.numeric), .SDcols = cols_to_convert]
skater_stats$TOI = gsub(",", "", skater_stats$TOI)
skater_stats[, TOI := as.numeric(TOI)]
skater_stats[, ATOI := TOI/GP]
skater_stats[is.na(skater_stats)] <- 0 # ez csinál mindent nullává, a többi innentől nem kell
# ebből még fgvt kell csinálni
# skater_stats$G[is.na(skater_stats$G)] <- 0
# skater_stats$PTS[is.na(skater_stats$PTS)] <- 0
# skater_stats$PlusMinus[is.na(skater_stats$PlusMinus)] <- 0
# skater_stats$A[is.na(skater_stats$A)] <- 0
# skater_stats$PIM[is.na(skater_stats$PIM)] <- 0
# skater_stats$EVG[is.na(skater_stats$EVG)] <- 0
# skater_stats$PPG[is.na(skater_stats$PPG)] <- 0
# skater_stats$SHG[is.na(skater_stats$SHG)] <- 0
# skater_stats$GWG[is.na(skater_stats$GWG)] <- 0
# skater_stats$EVA[is.na(skater_stats$EVA)] <- 0
# skater_stats$PPA[is.na(skater_stats$PPA)] <- 0
# skater_stats$SHA[is.na(skater_stats$SHA)] <- 0
# skater_stats$S[is.na(skater_stats$S)] <- 0
# skater_stats$S_perc[skater_stats$S == 0] <- NA
# skater_stats$BLK[is.na(skater_stats$BLK)] <- 0
# skater_stats$HIT[is.na(skater_stats$HIT)] <- 0
# skater_stats$FOwin[is.na(skater_stats$FO_perc)] <- NA
# skater_stats$FOloss[is.na(skater_stats$FO_perc)] <- NA
# # skater_stats <- skater_stats[!(Pos == "RW/LW")]
# # skater_stats <- skater_stats[!(Pos == "LW/RW")]
# # skater_stats <- skater_stats[!(Pos == "RW/C")]
# # skater_stats <- skater_stats[!(Pos == "C/LW")]
# # skater_stats <- skater_stats[!(Pos == "LW/C")]
# skater_stats <- skater_stats[!(Tm == "TOT")]
# # skater_stats <- skater_stats[(Age > 0)]

data <- skater_stats %>%
  group_by(Season, Tm) %>%
  summarise(Goals = sum(G),
            Age = mean(Age),
            GamesPlayed = mean(GP),
            Game_per_goal = mean(GPG),
            Assists = sum(A),
            Points = sum(PTS),
            PlusMinus_sum = sum(PlusMinus),
            PlusMinus_mean = mean(PlusMinus),
            Penalty_minutes = sum(PIM),
            Shots = sum(S),
            Time_on_ice = mean(TOI),
            Average_time_on_ice = mean(ATOI),
            Blocks = sum(BLK),
            Hits = sum(HIT)
            )
```

## Prediction

- We decided to use Ridge regression as there are a lot of high collinear variables in our dataset
- For this part we created the teaching and test groups
- We used correlation to throw out variables which were basically recording the same data

**változók értelmezése ide**

```{r, echo = FALSE}
index <- sample(1:331, 66)
test_data <- data[index,]
teach_data <- data[-index,]
```

```{r 1st try}
temp_teach <- subset(teach_data, select = -c(Season, Tm))
# cor(temp_teach)

# formula1 <- as.formula("Goals ~ Age + GamesPlayed + Game_per_goal +
#                        Assists + PlusMinus_mean + Penalty_minutes +
#                        Shots + Average_time_on_ice + Blocks + Hits")

x_var <- data.matrix(teach_data[, c("Age", "GamesPlayed", "Assists", "Points",
                                    "PlusMinus_mean", "Penalty_minutes", "Shots", 
                                    "Average_time_on_ice", "Blocks", "Hits")])
y_var <- data.matrix(teach_data[, "Goals"])

model_data <- as.data.frame(cbind(x_var, y_var))
names(model_data) <- c("Age", "GamesPlayed", "Assists", "Points", 
                       "PlusMinus_mean", "Penalty_minutes", "Shots", 
                       "Average_time_on_ice", "Blocks", "Hits", "Goals")

# linearmodel <- lm(Goals ~., data = model_data)
# test_data$PredictedGoals <- predict(linearmodel, newdata = test_data)

lambda_seq <- 10^seq(2, -2, by = -.1)

fit <- glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
ridge_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
# best_lambda
best_fit <- ridge_cv$glmnet.fit
# head(best_fit)

best_ridge <- glm(Goals ~., data = model_data)
coef(best_ridge)

test_data$PredictedGoals <- predict(best_ridge, newdata = test_data)
actual <- test_data$Goals
preds <- test_data$PredictedGoals
rss <- sum((preds - actual)^2)
tss <- sum((actual - mean(actual))^2)
rsq <- 1 - rss/tss
rsq

summary(best_ridge)
```

- As the r squared value was 1 we suspected there might be variables that  allow for perfect prediction
- After a while we realized that indeed that was the case, *(Substracting the number of Assists from the number of Points you get the number of Goals)* so we removed one of these varibles to give the model a challange
- First we removed *Assists* then as the r squared values was still too close to 1 we removed *Points* as well

```{r w/out Assists}
# Without Assists
x_var <- data.matrix(teach_data[, c("Age", "GamesPlayed", "Points",
                                    "PlusMinus_mean", "Penalty_minutes", "Shots", 
                                    "Average_time_on_ice", "Blocks", "Hits")])
model_data$Assists <- NULL

fit <- glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
ridge_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
# best_lambda
best_fit <- ridge_cv$glmnet.fit
# head(best_fit)

best_ridge <- glm(Goals ~., data = model_data)
coef(best_ridge)

test_data$PredictedGoals <- predict(best_ridge, newdata = test_data)
actual <- test_data$Goals
preds <- test_data$PredictedGoals
rss <- sum((preds - actual)^2)
tss <- sum((actual - mean(actual))^2)
rsq <- 1 - rss/tss
rsq

summary(best_ridge)
```
```{r w/out Assists and Points}
# Without Assists and Points

x_var <- data.matrix(teach_data[, c("Age", "GamesPlayed","PlusMinus_mean",
                                    "Penalty_minutes", "Shots", 
                                    "Average_time_on_ice", "Blocks", "Hits")])
model_data$Points <- NULL

fit <- glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
ridge_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
best_lambda <- ridge_cv$lambda.min
# best_lambda
best_fit <- ridge_cv$glmnet.fit
# head(best_fit)

best_ridge <- glm(Goals ~., data = model_data)
coef(best_ridge)

test_data$PredictedGoals <- predict(best_ridge, newdata = test_data)
actual <- test_data$Goals
preds <- test_data$PredictedGoals
rss <- sum((preds - actual)^2)
tss <- sum((actual - mean(actual))^2)
rsq <- 1 - rss/tss
rsq

summary(best_ridge)
```

```{r ábrák}
# olyan ábra, amin látszik, az összes gól (db) és más színnel a test adatsor
ggplot(teach_data,aes(Age,Goals)) +geom_point() +geom_point(data=test_data,colour='red')
# test valós gólok vs predicted gólok
# ami még eszünkbe jut
```

```{r ábrák}
skater_stats[PIM]
```